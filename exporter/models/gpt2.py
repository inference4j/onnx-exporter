import shutil
from pathlib import Path

from exporter.base import ExportedModel
from exporter.registry import register


class Gpt2(ExportedModel):
    name = "gpt2"
    repo_id = "inference4j/gpt2"

    def stage(self, staging_dir: Path) -> None:
        import numpy as np
        import onnxruntime as ort
        from huggingface_hub import hf_hub_download
        from optimum.exporters.onnx import main_export

        model_id = "openai-community/gpt2"

        # Export with optimum (merged model with KV cache)
        print("  Exporting GPT-2 to ONNX with KV cache...")
        main_export(
            model_name_or_path=model_id,
            output=staging_dir,
            task="text-generation-with-past",
            no_post_process=False,
        )

        # Rename merged model to model.onnx
        target = staging_dir / "model.onnx"
        if not target.exists():
            merged = staging_dir / "decoder_model_merged.onnx"
            if merged.exists():
                merged.rename(target)
                print("  Renamed decoder_model_merged.onnx -> model.onnx")
            else:
                # Fallback: use any .onnx file produced
                for candidate in sorted(staging_dir.glob("*.onnx")):
                    candidate.rename(target)
                    print(f"  Renamed {candidate.name} -> model.onnx")
                    break

        # Clean up intermediate model files generated by optimum
        for f in staging_dir.glob("decoder_model*.onnx"):
            f.unlink()
            print(f"  Removed {f.name}")

        # Copy tokenizer files from HuggingFace
        for filename in ("vocab.json", "merges.txt", "config.json"):
            dst = staging_dir / filename
            if not dst.exists():
                src = hf_hub_download(repo_id=model_id, filename=filename)
                shutil.copy2(src, dst)
                print(f"  Copied {filename}")

        # Remove non-essential files generated by optimum
        for name in ("tokenizer.json", "tokenizer_config.json",
                     "special_tokens_map.json", "generation_config.json",
                     "added_tokens.json"):
            p = staging_dir / name
            if p.exists():
                p.unlink()

        # Validate export
        print("  Validating export...")
        session = ort.InferenceSession(str(target))
        input_names = [i.name for i in session.get_inputs()]
        output_names = [o.name for o in session.get_outputs()]
        print(f"    Inputs: {input_names}")
        print(f"    Outputs: {output_names}")

        assert "input_ids" in input_names
        assert "attention_mask" in input_names
        assert any("past_key_values" in n for n in input_names), \
            f"No past_key_values in inputs: {input_names}"
        assert "logits" in output_names
        assert any("present" in n for n in output_names), \
            f"No present in outputs: {output_names}"

        # Quick inference test
        prompt_ids = np.array([[15496]], dtype=np.int64)  # "Hello"
        attention = np.ones_like(prompt_ids)
        feeds = {"input_ids": prompt_ids, "attention_mask": attention}

        # Add position_ids if expected
        if "position_ids" in input_names:
            feeds["position_ids"] = np.zeros_like(prompt_ids)

        # Add empty KV cache
        for inp in session.get_inputs():
            if inp.name.startswith("past_key_values"):
                shape = list(inp.shape)
                # Replace dynamic dims with 0 (empty cache)
                shape = [s if isinstance(s, int) else 0 for s in shape]
                shape[0] = 1  # batch size
                feeds[inp.name] = np.zeros(shape, dtype=np.float32)

        results = session.run(None, feeds)
        logits = results[0]
        print(f"    Logits shape: {logits.shape}")
        assert logits.shape[-1] == 50257, f"Expected vocab size 50257, got {logits.shape[-1]}"
        print("  Validation OK")

        size_mb = target.stat().st_size / (1024 * 1024)
        print(f"  Export complete: model.onnx ({size_mb:.1f} MB)")

    def render_card(self) -> str:
        return """\
---
library_name: onnx
tags:
  - text-generation
  - gpt2
  - onnx
  - inference4j
license: mit
pipeline_tag: text-generation
---

# GPT-2 â€” ONNX

ONNX export of [GPT-2](https://huggingface.co/openai-community/gpt2) (124M parameters) \
with KV cache support for efficient autoregressive generation.

Converted for use with [inference4j](https://github.com/inference4j/inference4j), \
an inference-only AI library for Java.

## Original Source

- **Repository:** [OpenAI](https://huggingface.co/openai-community/gpt2)
- **License:** MIT

## Usage with inference4j

```java
try (Gpt2TextGenerator gen = Gpt2TextGenerator.builder().build()) {
    GenerationResult result = gen.generate("Once upon a time");
    System.out.println(result.text());
}
```

## Model Details

| Property | Value |
|----------|-------|
| Architecture | GPT-2 (124M parameters, 12 layers, 768 hidden, 12 heads) |
| Task | Text generation |
| Context length | 1024 tokens |
| Vocabulary | 50257 tokens (BPE) |
| Original framework | PyTorch (transformers) |
| Export method | Hugging Face Optimum (with KV cache) |

## License

This model is licensed under the [MIT License](https://opensource.org/licenses/MIT). \
Original model by [OpenAI](https://openai.com/).
"""


register(Gpt2())
